# Collecting Argo Workflow Logs Using Fluentd
### What is Argo Workflow?
As described in [here][1], Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo Workflows is implemented as a Kubernetes CRD (Custom Resource Definition).Here are what we can do using Argo:

* Define workflows where each step in the workflow is a container.
* Model multi-step workflows as a sequence of tasks or capture the dependencies between tasks using a directed acyclic graph (DAG).
* Easily run compute intensive jobs for machine learning or data processing in a fraction of the time using Argo Workflows on Kubernetes.
* Argo is a Cloud Native Computing Foundation (CNCF) graduated project.

First, let's install and learn the basics of using Argo Workflows. The following materials are mainly from this [Killercoda course][2] which is created by Argo creaters themseslves!

### 1. Installing Argo Workflows on Minikube
```%sh
# creating a new minikube cluster called argo
$ minikube start -p argo

# setting the default minikube profile to argo
$ minikube profile argo

# setting the kubetl context to our new minikube argo cluster
$kubectl config use-context argo

# creating a new namespace to host argo deployments and services
$kubectl create namespace argo

# installing argo workflows
$kubectl apply -n argo -f https://github.com/argoproj/argo-workflows/releases/download/v3.4.9/install.yaml
```
Two deployments would have to be installed if the above has been executed successfully:

```
$ kubectl get deployments -n argo

NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
argo-server           1/1     1            1           18m
workflow-controller   1/1     1            1           18m
```

The *__Workflow Controller__* is responsible for running workflows and the *__Argo Server__* provides a user interface and API.

### 2. Creating and Running a Simple Workflow
A workflow is defined as a Kubernetes resource. Each workflow consists of one or more templates, one of which is defined as the entrypoint. Each template can be one of several types, in this example we have one template that is a container.

The ```hello-workflow.yaml``` in this directory contains one template that is a container. 

```%sh
# creating a workflow
$ kubectl -n argo apply -f hello-workflow.yaml
```

Preferably, we use Argo CLI to run workflows, instead of kubectl:
```%sh
# submitting a workflow using Argo CLI
$ argo submit -n argo --watch hello-workflow.yaml
```

We can get more info and details about the workflows using the following commands:
```%sh
# listing workflows
$ argo list -n argo

# getting details of a workflow
$ argo get -n argo hello

#getting logs generated by a workflow
$ argo logs -n argo hello
```

### 3. Templates [to be changed]
There are several types of templates, divided into two different categories: *work* and *orchestration*. The first category defines work to be done. This includes:
* Container
* Container Set
* Data
* Resource
* Script

The second category orchestrates the work:
* DAG
* Steps
* Suspend

We've already submitted a workflow containing single container template in part 2 (```hello-workflow.yaml```)

A __*container set*__ allows you to run multiple containers in a single pod. This is useful when you want the containers to share a common workspace, or when you want to consolidate pod spin-up time into one step in your workflow.

A *__data template__* allows you to get data from storage (e.g. S3). This is useful when each item of data represents an item of work that needs doing.

A __*resource template*__ allows you to create a Kubernetes resource and wait for it to meet a condition (e.g. successful). This is useful if you want to interoperate with another Kubernetes system, like AWS Spark EMR

A *__script template__* allows you to run a script in a container. This is very similar to a container template, but when you've added a script to it.

A *__DAG template__* is a common type of orchestration template. Look at ```dag-workflow.yaml``` as an example. 

```%sh
$ argo -n argo submit --watch dag-workflow.yaml
```

You should see something like:
```
Name:                dag-nms8c
Namespace:           argo
ServiceAccount:      unset (will run with the default ServiceAccount)
Status:              Succeeded
Conditions:          
 PodRunning          False
 Completed           True
Created:             Mon Jul 24 15:14:36 -0400 (20 seconds ago)
Started:             Mon Jul 24 15:14:36 -0400 (20 seconds ago)
Finished:            Mon Jul 24 15:14:56 -0400 (now)
Duration:            20 seconds
Progress:            2/2
ResourcesDuration:   6s*(1 cpu),6s*(100Mi memory)

STEP          TEMPLATE  PODNAME                        DURATION  MESSAGE
 ✔ dag-nms8c  main                                                 
 ├─✔ a        whalesay  dag-nms8c-whalesay-4009067076  4s          
 └─✔ b        whalesay  dag-nms8c-whalesay-4059399933  3s  
```
Explore [[2]] to learn more details about Argo Workflows.

[1]: https://argoproj.github.io/argo-workflows/
[2]: https://killercoda.com/pipekit/course/argo-workflows/

